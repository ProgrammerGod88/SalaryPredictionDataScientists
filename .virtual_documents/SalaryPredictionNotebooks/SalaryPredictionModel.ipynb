# Data manipulation and analysis
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn for preprocessing and model evaluation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline

# TensorFlow and Keras for building the deep learning model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, LeakyReLU, Input
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Optional: XGBoost for model comparison (if needed)
import xgboost as xgb

# Set visualization styles
sns.set(style="whitegrid")


df = pd.read_csv("/mnt/d/SalaryPredictionDataScientists/SalariesData/DataScience_salaries_2024.csv")


df.head()


# Check for missing values and data types
print("Dataset information:")
df.info()


print("Summary statistics:")
df.describe()


# Check for missing values in each column
print("Missing values per column:")
print(df.isnull().sum())


#Pre-processing if needed for the given dataset

# Define columns based on data type
numerical_features = ['salary_in_usd']  # Add any other numerical columns if they exist
categorical_features = ['experience_level', 'employment_type', 'job_title', 'employee_residence']

# Fill missing numerical values with the median, and categorical with the most frequent value
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformations into a preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)


# Transform categorical variables and prepare feature matrix X and target variable y
X = df.drop(columns=['salary'])  # Remove target column from features
y = df['salary_in_usd']  # Define target variable

# Apply transformations to features
X_prepared = preprocessor.fit_transform(X)

# Check the shape of the transformed features
print("Shape of transformed features:", X_prepared.shape)


# Standardize the numerical features with with_mean=False
scaler = StandardScaler(with_mean=False)  # Set with_mean=False to handle sparse matrices
X_prepared = scaler.fit_transform(X_prepared)

# Check the first few rows to confirm transformation
print("Transformed feature matrix:")
print(X_prepared[:5])


# Split data into training and test sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X_prepared, y, test_size=0.2, random_state=42)

# Check the shape of each set
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)



# Save processed data to .npz files
np.savez("processed_data.npz", X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)



# Define the model architecture
model = Sequential([
    Input(shape=(X_train.shape[1],)), 
    Dense(256, kernel_regularizer=l2(0.001)),
    LeakyReLU(negative_slope=0.1), 
    Dropout(0.3),
    
    Dense(128, kernel_regularizer=l2(0.001)),
    LeakyReLU(negative_slope=0.1), 
    Dropout(0.3),
    
    Dense(64, kernel_regularizer=l2(0.001)),
    LeakyReLU(negative_slope=0.1),  
    Dense(1) 
])


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0005),
              loss='mean_squared_error',
              metrics=['mean_absolute_error'])


# Train the model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    verbose=1
)


# Evaluate the model on the test set
test_loss, test_mae = model.evaluate(X_test, y_test)
print(f"Test Mean Absolute Error: {test_mae}")


# Plot training & validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()



